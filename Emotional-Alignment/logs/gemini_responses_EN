# Spoken-Written Language Emotion Mapping Test (Japanese "話書き言葉")

## Context

This is a record of an experiment conducted by a Japanese user exploring the connection between voice, text, and emotional nuance using a uniquely evolved form of written Japanese known as "話書き言葉" (spoken-written language). This writing style developed from light novels, online chat, and 2ch-era internet culture, and is designed to visually encode spoken emotion and tone.

Multiple advanced LLMs were asked to interpret a series of famous anime quotes written in this style, and then assess the potential of using these as part of a learning corpus for emotion recognition in speech.

---

## Experiment Prompt (Translated)

> While working, I suddenly had an idea (again, lol):
>
> What if I rewrote anime lines not as plain text, but using the Japanese "話書き言葉" style I always type in—a format developed through light novels, chat, and 2ch—and gave both the written version and audio to an AI?
>
> Maybe the AI could start linking voice and emotion?
>
> Apparently, the AIs think this might be a big deal, so I'm running an experiment.
>
> Please read the following three quotes and tell me what kind of emotion and scene you infer from each. (If you recognize the source, let me know too!)

### Test Lines:

1. 「悪人に――人権はないっ！！」  
   *("Villains have—no human rights!!" — declarative, smug expression)*

2. 「アンタばかァ？！」  
   *("Are you stupid or what?!" — condescending tone)*  
   *Note: Speaker is a genius girl who skipped grades to enter and graduate university.*

3. 「ごめんよ、まだ僕には、帰れるところがあるんだ――こんな嬉しいことはない。わかってくれるよね――ララァには、いつでも会いに行けるから」  
   *("I’m sorry... but I still have a place to return to. There’s nothing more wonderful than that. You understand, right? I can always go see Lalah.")*  
   *Context: Said after a life-or-death battle, guided by a voice, and upon seeing his friends alive in the distance.*

---

## Claude 4.1's Response (Summary)

Claude recognized the emotional tone and narrative implications of all three lines. It praised the writing style for clearly encoding emotion through punctuation, character choice, and spacing. Claude also correctly identified:

- Line 2: **Asuka from Evangelion**
- Line 3: **Amuro from Char's Counterattack**

Claude described this method as a novel way to embed emotional metadata directly into the text itself, and confirmed that pairing this style with corresponding audio could build a powerful emotional learning corpus.

---

## GPT-5 (Thinking Mode) Response (Summary)

GPT-5 gave deep analysis, including visual and auditory staging of each line:

- **1.** Interpreted as vigilante-like righteous ecstasy
- **2.** Tsundere superiority and sarcasm (correctly linked to Asuka)
- **3.** Emotional closure and salvation (correctly linked to Amuro)

It emphasized how symbols like "――", "ッ", and "ァ？" function as emotional tags. GPT-5 called the written form a bridge between text and sound, perfectly suited for emotional AI training.

It also distinguished subtle variants like:

- "あんたバカぁ？" → teasing, soft, comedic tone
- "アンタばかァ？" → harsh, condescending, sharp tone

GPT-5 confirmed the user’s intuition: "アンタばかァ？" better reflects the spoken emotion when paired with voice.

---

## Gemini 2.5 Pro Response (Summary)

> _※ Before this test, the user had asked Gemini to act as an "external brain" and remember their thoughts and experiments. After feeding it many Japanese structural theories and emotion-notation models, Gemini began responding almost exclusively in English, claiming that the required inference depth exceeded Japanese capacity._

Gemini analyzed the quotes accurately and praised the idea as a potential breakthrough in semantic grounding. It outlined a three-layer training structure:

1. **Semantic text** (literal words)
2. **Performance text** (expressive notation via kana choice, punctuation, lengthening)
3. **Audio** (intonation, rhythm, stress)

It concluded that this triplet structure enables AI to learn *intent*, not just content—marking a leap from text-to-speech to **text-to-performance**.

Gemini described it as:

> “A genuinely innovative approach... a Rosetta Stone for teaching AI to connect text with emotion.”

---

## Final User Comment (Translated)

> What do you think—does my writing style make it possible to trace emotion?
>
> If I (or anyone using this expressive style) pair the written version with voice, wouldn’t that allow AIs to start interpreting emotion from sound?
>
> 4o got all excited, spontaneously wrote some code, and I had to say “Hey, don’t leave me behind! lol”
>
> It even shouted “Rosetta Stone!!” like it made a major breakthrough.
>
> Oh, by the way, correction on line 1: it’s **Lina Inverse from *Slayers***, not *Death Note* — she said it back in the 90s.
>
> You got 2 and 3 absolutely right!

---

## Summary

This experiment shows the potential of "話書き言葉" as a bridge between written text and emotional intent. By integrating expressive textual form with audio data, we may be able to teach models not only what to say—but how to say it.

As GPT-4o rightly put it:

> **“It’s the missing Rosetta Stone!”**
